{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Interactive SAE Pipeline Tutorial\n",
        "\n",
        "This notebook provides an interactive walkthrough of the Sparse Autoencoder (SAE) training pipeline, replicating the functionality of `SAETrainingPipeline.run_complete_pipeline()` from `sae_pipeline.py`.\n",
        "\n",
        "## Overview\n",
        "\n",
        "The SAE pipeline consists of several key steps:\n",
        "1. **Embedding Generation**: Extract embeddings from HelicalmRNA model\n",
        "2. **Data Preparation**: Create training and validation dataloaders\n",
        "3. **Model Setup**: Initialize the Sparse Autoencoder\n",
        "4. **Training**: Train the model with sparsity constraints\n",
        "5. **Visualization**: Plot training progress and results\n",
        "\n",
        "Let's explore each step interactively!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Add the src directory to the path\n",
        "notebook_dir = Path.cwd()\n",
        "src_path = notebook_dir.parent / \"src\"\n",
        "sys.path.append(str(src_path))\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "# Import SAE components\n",
        "from sae.pipeline.sae_pipeline import SAETrainingPipeline\n",
        "from sae.models.sae import SAE\n",
        "from sae.training.trainer import SAETrainer, TrainingConfig\n",
        "from sae.losses.losses import SAELoss\n",
        "\n",
        "# Set up plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"‚úÖ Imports completed successfully!\")\n",
        "print(f\"üìÅ Working directory: {notebook_dir}\")\n",
        "print(f\"üìÅ Source path: {src_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Set up the pipeline parameters. You can modify these values to experiment with different configurations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pipeline configuration\n",
        "config = {\n",
        "    'refseq_file': '../data/vertebrate_mammalian.1.rna.gbff',  # Path to your RefSeq file\n",
        "    'max_samples': 200,  # Number of samples to process (smaller for faster demo)\n",
        "    'hidden_dim': 500,   # Number of SAE features to learn\n",
        "    'epochs': 15,        # Number of training epochs\n",
        "    'batch_size': 8,     # Batch size for training\n",
        "    'sparsity_weight': 0.01,  # Weight for sparsity penalty\n",
        "    'learning_rate': 0.001,   # Learning rate\n",
        "    'layer_idx': None,   # Layer index to extract embeddings from (None for final layer)\n",
        "    'layer_name': 'final'  # Layer name for identification\n",
        "}\n",
        "\n",
        "print(\"üîß Pipeline Configuration:\")\n",
        "for key, value in config.items():\n",
        "    print(f\"   {key}: {value}\")\n",
        "\n",
        "# Check if RefSeq file exists\n",
        "refseq_path = Path(config['refseq_file'])\n",
        "if refseq_path.exists():\n",
        "    print(f\"\\n‚úÖ RefSeq file found: {refseq_path}\")\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è  RefSeq file not found: {refseq_path}\")\n",
        "    print(\"   Please update the 'refseq_file' path in the config above.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Initialize Pipeline\n",
        "\n",
        "Create the SAE training pipeline with the specified configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the SAE training pipeline\n",
        "pipeline = SAETrainingPipeline(\n",
        "    embedding_dim=None,  # Will be auto-detected from embeddings\n",
        "    hidden_dim=config['hidden_dim'],\n",
        "    sparsity_weight=config['sparsity_weight'],\n",
        "    learning_rate=config['learning_rate'],\n",
        "    layer_idx=config['layer_idx'],\n",
        "    layer_name=config['layer_name']\n",
        ")\n",
        "\n",
        "print(\"‚úÖ SAE Training Pipeline initialized!\")\n",
        "print(f\"   Hidden dimension: {config['hidden_dim']}\")\n",
        "print(f\"   Sparsity weight: {config['sparsity_weight']}\")\n",
        "print(f\"   Learning rate: {config['learning_rate']}\")\n",
        "print(f\"   Layer: {config['layer_name']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Setup Embedding Generator\n",
        "\n",
        "Initialize the embedding generator to extract embeddings from the HelicalmRNA model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üîß Setting up embedding generator...\")\n",
        "\n",
        "# Setup embedding generator\n",
        "pipeline.setup_embedding_generator()\n",
        "\n",
        "print(\"‚úÖ Embedding generator setup complete!\")\n",
        "print(f\"   Model: {pipeline.embedding_generator.model_name}\")\n",
        "print(f\"   Wrapper: {pipeline.embedding_generator.wrapper_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Prepare Training Data\n",
        "\n",
        "Generate embeddings from the RefSeq file and create training/validation dataloaders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üîß Preparing training data...\")\n",
        "\n",
        "# Prepare data\n",
        "train_loader, val_loader = pipeline.prepare_data(\n",
        "    refseq_file=config['refseq_file'],\n",
        "    max_samples=config['max_samples'],\n",
        "    batch_size=config['batch_size'],\n",
        "    filter_by_type=\"mRNA\",\n",
        "    use_cds=True,\n",
        "    dataset_name=\"Interactive_Demo\"\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Data preparation complete!\")\n",
        "print(f\"   Embedding dimension: {pipeline.embedding_dim}\")\n",
        "print(f\"   Training batches: {len(train_loader)}\")\n",
        "print(f\"   Validation batches: {len(val_loader)}\")\n",
        "print(f\"   Batch size: {config['batch_size']}\")\n",
        "\n",
        "# Show sample data shape\n",
        "for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    print(f\"\\nüìä Sample batch {batch_idx + 1}:\")\n",
        "    print(f\"   Input shape: {data.shape}\")\n",
        "    print(f\"   Target shape: {target.shape}\")\n",
        "    print(f\"   Data type: {data.dtype}\")\n",
        "    print(f\"   Value range: [{data.min():.3f}, {data.max():.3f}]\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Setup SAE Model\n",
        "\n",
        "Initialize the Sparse Autoencoder model with the specified architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üîß Setting up SAE model...\")\n",
        "\n",
        "# Setup SAE model\n",
        "pipeline.setup_sae_model()\n",
        "\n",
        "print(\"‚úÖ SAE model setup complete!\")\n",
        "print(f\"   Input size: {pipeline.embedding_dim}\")\n",
        "print(f\"   Hidden size: {pipeline.hidden_dim}\")\n",
        "print(f\"   Model parameters: {sum(p.numel() for p in pipeline.sae_model.parameters()):,}\")\n",
        "\n",
        "# Test forward pass with sample data\n",
        "sample_data, _ = next(iter(train_loader))\n",
        "with torch.no_grad():\n",
        "    reconstructed, encoded = pipeline.sae_model(sample_data)\n",
        "    \n",
        "print(f\"\\nüß™ Forward pass test:\")\n",
        "print(f\"   Input shape: {sample_data.shape}\")\n",
        "print(f\"   Encoded shape: {encoded.shape}\")\n",
        "print(f\"   Reconstructed shape: {reconstructed.shape}\")\n",
        "print(f\"   Reconstruction MSE: {torch.nn.functional.mse_loss(reconstructed, sample_data):.6f}\")\n",
        "\n",
        "# Calculate sparsity in encoded representation\n",
        "sparsity = (encoded == 0).float().mean()\n",
        "print(f\"   Initial sparsity: {sparsity:.3f} ({sparsity*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Setup Trainer\n",
        "\n",
        "Configure the training setup with optimizer, loss function, and training configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üîß Setting up trainer...\")\n",
        "\n",
        "# Setup trainer\n",
        "pipeline.setup_trainer(train_loader, val_loader)\n",
        "\n",
        "print(\"‚úÖ Trainer setup complete!\")\n",
        "print(f\"   Optimizer: {type(pipeline.trainer.optimizer).__name__}\")\n",
        "print(f\"   Learning rate: {pipeline.trainer.optimizer.param_groups[0]['lr']}\")\n",
        "print(f\"   Loss function: {type(pipeline.trainer.loss_fn).__name__}\")\n",
        "print(f\"   Sparsity weight: {pipeline.trainer.loss_fn.sparsity_weight}\")\n",
        "print(f\"   Sparsity target: {pipeline.trainer.loss_fn.sparsity_target}\")\n",
        "\n",
        "# Test loss computation\n",
        "sample_data, _ = next(iter(train_loader))\n",
        "with torch.no_grad():\n",
        "    loss, loss_dict = pipeline.trainer.loss_fn(\n",
        "        pipeline.sae_model(sample_data)[0],  # reconstructed\n",
        "        sample_data,  # target\n",
        "        pipeline.sae_model(sample_data)[1]   # encoded\n",
        "    )\n",
        "    \n",
        "print(f\"\\nüß™ Loss computation test:\")\n",
        "print(f\"   Total loss: {loss:.6f}\")\n",
        "for key, value in loss_dict.items():\n",
        "    print(f\"   {key}: {value:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Training\n",
        "\n",
        "Train the SAE model and monitor the training progress with detailed metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"üöÄ Starting training for {config['epochs']} epochs...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Train the model\n",
        "history = pipeline.train(epochs=config['epochs'])\n",
        "\n",
        "print(\"\\n‚úÖ Training completed!\")\n",
        "print(f\"   Total epochs: {len(history)}\")\n",
        "print(f\"   Final training loss: {history[-1]['total_loss']:.6f}\")\n",
        "print(f\"   Final validation loss: {history[-1].get('val_total_loss', 'N/A')}\")\n",
        "if 'val_l0_sparsity' in history[-1]:\n",
        "    print(f\"   Final L0 sparsity: {history[-1]['val_l0_sparsity']:.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Training Visualization\n",
        "\n",
        "Create comprehensive plots to visualize the training progress and results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert history to DataFrame for easier plotting\n",
        "history_df = pd.DataFrame(history)\n",
        "history_df['epoch'] = range(1, len(history_df) + 1)\n",
        "\n",
        "print(\"üìä Training History Summary:\")\n",
        "print(history_df[['epoch', 'total_loss', 'reconstruction_loss', 'sparsity_loss']].tail())\n",
        "\n",
        "# Create comprehensive training plots\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "fig.suptitle(f'SAE Training Progress - {config[\"hidden_dim\"]} Features, {config[\"epochs\"]} Epochs', fontsize=16)\n",
        "\n",
        "# 1. Total Loss\n",
        "ax1 = axes[0, 0]\n",
        "ax1.plot(history_df['epoch'], history_df['total_loss'], 'b-', linewidth=2, label='Training')\n",
        "if 'val_total_loss' in history_df.columns:\n",
        "    ax1.plot(history_df['epoch'], history_df['val_total_loss'], 'r--', linewidth=2, label='Validation')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Total Loss')\n",
        "ax1.set_title('Total Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Reconstruction Loss\n",
        "ax2 = axes[0, 1]\n",
        "ax2.plot(history_df['epoch'], history_df['reconstruction_loss'], 'g-', linewidth=2, label='Training')\n",
        "if 'val_reconstruction_loss' in history_df.columns:\n",
        "    ax2.plot(history_df['epoch'], history_df['val_reconstruction_loss'], 'm--', linewidth=2, label='Validation')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Reconstruction Loss')\n",
        "ax2.set_title('Reconstruction Loss')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Sparsity Loss\n",
        "ax3 = axes[0, 2]\n",
        "ax3.plot(history_df['epoch'], history_df['sparsity_loss'], 'orange', linewidth=2, label='Training')\n",
        "if 'val_sparsity_loss' in history_df.columns:\n",
        "    ax3.plot(history_df['epoch'], history_df['val_sparsity_loss'], 'brown', linewidth=2, label='Validation')\n",
        "ax3.set_xlabel('Epoch')\n",
        "ax3.set_ylabel('Sparsity Loss')\n",
        "ax3.set_title('Sparsity Loss')\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# 4. L0 Sparsity (if available)\n",
        "ax4 = axes[1, 0]\n",
        "if 'val_l0_sparsity' in history_df.columns:\n",
        "    ax4.plot(history_df['epoch'], history_df['val_l0_sparsity'], 'purple', linewidth=2, marker='o')\n",
        "    ax4.set_xlabel('Epoch')\n",
        "    ax4.set_ylabel('L0 Sparsity')\n",
        "    ax4.set_title('L0 Sparsity (Non-zero features per token)')\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "else:\n",
        "    ax4.text(0.5, 0.5, 'L0 Sparsity\n",
        "Not Available', ha='center', va='center', transform=ax4.transAxes)\n",
        "    ax4.set_title('L0 Sparsity')\n",
        "\n",
        "# 5. Loss Components Comparison\n",
        "ax5 = axes[1, 1]\n",
        "ax5.plot(history_df['epoch'], history_df['reconstruction_loss'], 'g-', linewidth=2, label='Reconstruction')\n",
        "ax5.plot(history_df['epoch'], history_df['sparsity_loss'], 'orange', linewidth=2, label='Sparsity')\n",
        "ax5.set_xlabel('Epoch')\n",
        "ax5.set_ylabel('Loss')\n",
        "ax5.set_title('Loss Components')\n",
        "ax5.legend()\n",
        "ax5.grid(True, alpha=0.3)\n",
        "\n",
        "# 6. Final Loss Comparison\n",
        "ax6 = axes[1, 2]\n",
        "final_epoch = history_df.iloc[-1]\n",
        "loss_types = ['reconstruction_loss', 'sparsity_loss']\n",
        "loss_values = [final_epoch['reconstruction_loss'], final_epoch['sparsity_loss']]\n",
        "colors = ['green', 'orange']\n",
        "\n",
        "bars = ax6.bar(loss_types, loss_values, color=colors, alpha=0.7)\n",
        "ax6.set_ylabel('Loss Value')\n",
        "ax6.set_title('Final Loss Breakdown')\n",
        "ax6.grid(True, alpha=0.3)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, value in zip(bars, loss_values):\n",
        "    ax6.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
        "             f'{value:.4f}', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Model Analysis\n",
        "\n",
        "Analyze the trained model to understand the learned representations and sparsity patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze the trained model\n",
        "print(\"üîç Model Analysis\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Test the model on validation data\n",
        "pipeline.sae_model.eval()\n",
        "val_reconstruction_losses = []\n",
        "val_sparsity_values = []\n",
        "val_l0_values = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, target in val_loader:\n",
        "        reconstructed, encoded = pipeline.sae_model(data)\n",
        "        \n",
        "        # Calculate reconstruction loss\n",
        "        recon_loss = torch.nn.functional.mse_loss(reconstructed, target)\n",
        "        val_reconstruction_losses.append(recon_loss.item())\n",
        "        \n",
        "        # Calculate sparsity\n",
        "        sparsity = (encoded == 0).float().mean()\n",
        "        val_sparsity_values.append(sparsity.item())\n",
        "        \n",
        "        # Calculate L0 sparsity (non-zero features per token)\n",
        "        if len(encoded.shape) == 3:\n",
        "            # (batch_size, seq_len, hidden_dim)\n",
        "            non_zero_per_token = (encoded != 0).sum(dim=2).float()  # (batch_size, seq_len)\n",
        "            l0_val = non_zero_per_token.mean()\n",
        "        else:\n",
        "            # (batch_size, hidden_dim)\n",
        "            l0_val = (encoded != 0).sum(dim=1).float().mean()\n",
        "        val_l0_values.append(l0_val.item())\n",
        "\n",
        "print(f\"üìä Validation Results:\")\n",
        "print(f\"   Average reconstruction loss: {np.mean(val_reconstruction_losses):.6f}\")\n",
        "print(f\"   Average sparsity: {np.mean(val_sparsity_values):.3f} ({np.mean(val_sparsity_values)*100:.1f}%)\")\n",
        "print(f\"   Average L0 sparsity: {np.mean(val_l0_values):.1f} features per token\")\n",
        "\n",
        "# Visualize learned features\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "fig.suptitle('Learned SAE Features Analysis', fontsize=16)\n",
        "\n",
        "# 1. Feature activation distribution\n",
        "ax1 = axes[0, 0]\n",
        "sample_data, _ = next(iter(val_loader))\n",
        "with torch.no_grad():\n",
        "    _, encoded = pipeline.sae_model(sample_data)\n",
        "    encoded_flat = encoded.flatten().cpu().numpy()\n",
        "    \n",
        "ax1.hist(encoded_flat, bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
        "ax1.set_xlabel('Feature Activation Value')\n",
        "ax1.set_ylabel('Frequency')\n",
        "ax1.set_title('Feature Activation Distribution')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.axvline(x=0, color='red', linestyle='--', alpha=0.7, label='Zero threshold')\n",
        "ax1.legend()\n",
        "\n",
        "# 2. Sparsity per sample\n",
        "ax2 = axes[0, 1]\n",
        "with torch.no_grad():\n",
        "    _, encoded = pipeline.sae_model(sample_data)\n",
        "    if len(encoded.shape) == 3:\n",
        "        # Calculate sparsity per sample\n",
        "        sparsity_per_sample = (encoded == 0).float().mean(dim=(1, 2)).cpu().numpy()\n",
        "    else:\n",
        "        sparsity_per_sample = (encoded == 0).float().mean(dim=1).cpu().numpy()\n",
        "    \n",
        "ax2.hist(sparsity_per_sample, bins=20, alpha=0.7, color='green', edgecolor='black')\n",
        "ax2.set_xlabel('Sparsity per Sample')\n",
        "ax2.set_ylabel('Frequency')\n",
        "ax2.set_title('Sparsity Distribution Across Samples')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Feature usage heatmap (top 20 most active features)\n",
        "ax3 = axes[1, 0]\n",
        "with torch.no_grad():\n",
        "    _, encoded = pipeline.sae_model(sample_data)\n",
        "    if len(encoded.shape) == 3:\n",
        "        # Average across sequence dimension\n",
        "        feature_activity = encoded.mean(dim=1)  # (batch_size, hidden_dim)\n",
        "    else:\n",
        "        feature_activity = encoded\n",
        "    \n",
        "    # Get top 20 most active features\n",
        "    feature_usage = feature_activity.abs().mean(dim=0)  # (hidden_dim,)\n",
        "    top_features = torch.topk(feature_usage, min(20, len(feature_usage))).indices\n",
        "    \n",
        "    # Create heatmap\n",
        "    heatmap_data = feature_activity[:, top_features].cpu().numpy()\n",
        "    im = ax3.imshow(heatmap_data.T, aspect='auto', cmap='viridis')\n",
        "    ax3.set_xlabel('Sample Index')\n",
        "    ax3.set_ylabel('Feature Index')\n",
        "    ax3.set_title('Top 20 Most Active Features')\n",
        "    plt.colorbar(im, ax=ax3)\n",
        "\n",
        "# 4. L0 sparsity over time (if sequence data)\n",
        "ax4 = axes[1, 1]\n",
        "with torch.no_grad():\n",
        "    _, encoded = pipeline.sae_model(sample_data)\n",
        "    if len(encoded.shape) == 3:\n",
        "        # Calculate L0 per sequence position\n",
        "        l0_per_position = (encoded != 0).sum(dim=2).float().mean(dim=0).cpu().numpy()\n",
        "        positions = range(len(l0_per_position))\n",
        "        ax4.plot(positions, l0_per_position, 'purple', linewidth=2, marker='o')\n",
        "        ax4.set_xlabel('Sequence Position')\n",
        "        ax4.set_ylabel('L0 Sparsity')\n",
        "        ax4.set_title('L0 Sparsity Across Sequence Positions')\n",
        "        ax4.grid(True, alpha=0.3)\n",
        "    else:\n",
        "        ax4.text(0.5, 0.5, 'Sequence data\n",
        "not available', ha='center', va='center', transform=ax4.transAxes)\n",
        "        ax4.set_title('L0 Sparsity Across Sequence')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Save Model and Results\n",
        "\n",
        "Save the trained model and training history for future use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the trained model\n",
        "save_dir = Path(\"../outputs/interactive_demo\")\n",
        "save_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "model_path = save_dir / \"sae_model_interactive.pth\"\n",
        "history_path = save_dir / \"training_history_interactive.json\"\n",
        "config_path = save_dir / \"model_config_interactive.json\"\n",
        "\n",
        "# Save model\n",
        "torch.save({\n",
        "    'model_state_dict': pipeline.sae_model.state_dict(),\n",
        "    'embedding_dim': pipeline.embedding_dim,\n",
        "    'hidden_dim': pipeline.hidden_dim,\n",
        "    'sparsity_weight': config['sparsity_weight'],\n",
        "    'layer_idx': config['layer_idx'],\n",
        "    'layer_name': config['layer_name']\n",
        "}, model_path)\n",
        "\n",
        "# Save training history\n",
        "import json\n",
        "with open(history_path, 'w') as f:\n",
        "    json.dump(history, f, indent=2)\n",
        "\n",
        "# Save configuration\n",
        "with open(config_path, 'w') as f:\n",
        "    json.dump(config, f, indent=2)\n",
        "\n",
        "print(\"üíæ Model and results saved!\")\n",
        "print(f\"   Model: {model_path}\")\n",
        "print(f\"   History: {history_path}\")\n",
        "print(f\"   Config: {config_path}\")\n",
        "\n",
        "# Print final summary\n",
        "print(\"\\nüéâ Interactive SAE Pipeline Complete!\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"üìä Final Results:\")\n",
        "print(f\"   Model: {pipeline.embedding_dim} ‚Üí {pipeline.hidden_dim} features\")\n",
        "print(f\"   Training epochs: {len(history)}\")\n",
        "print(f\"   Final reconstruction loss: {history[-1]['reconstruction_loss']:.6f}\")\n",
        "print(f\"   Final sparsity loss: {history[-1]['sparsity_loss']:.6f}\")\n",
        "if 'val_l0_sparsity' in history[-1]:\n",
        "    print(f\"   Final L0 sparsity: {history[-1]['val_l0_sparsity']:.1f} features per token\")\n",
        "print(f\"   Model parameters: {sum(p.numel() for p in pipeline.sae_model.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This interactive notebook has demonstrated the complete SAE training pipeline:\n",
        "\n",
        "1. **Data Loading**: Successfully loaded and processed RefSeq data\n",
        "2. **Embedding Generation**: Extracted embeddings from HelicalmRNA model\n",
        "3. **Model Architecture**: Set up a sparse autoencoder with tied weights\n",
        "4. **Training**: Trained the model with sparsity constraints\n",
        "5. **Analysis**: Visualized training progress and learned features\n",
        "6. **Results**: Achieved sparse representations with good reconstruction quality\n",
        "\n",
        "### Key Insights:\n",
        "- The model learned sparse representations with controlled sparsity\n",
        "- L0 sparsity shows how many features are active per token\n",
        "- Training history reveals the trade-off between reconstruction and sparsity\n",
        "- Feature analysis shows which dimensions are most important\n",
        "\n",
        "### Next Steps:\n",
        "- Experiment with different `hidden_dim` values\n",
        "- Adjust `sparsity_weight` to control sparsity levels\n",
        "- Try different layers of the HelicalmRNA model\n",
        "- Compare with BatchTopK SAE for different sparsity patterns\n",
        "\n",
        "The saved model can be loaded and used for feature extraction or further analysis!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
